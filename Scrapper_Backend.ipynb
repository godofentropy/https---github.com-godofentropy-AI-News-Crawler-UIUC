{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "USER_KEYWORDS = []  # User can specify custom keywords here\n",
    "AI_KEYWORDS = [\"Gen-AI\", \"Generative AI\", \"Artificial Intelligence\", \"Machine Learning\"] + USER_KEYWORDS\n",
    "\n",
    "# Manually defined stopwords to avoid NLTK download issues\n",
    "STOPWORDS = {\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"for\", \"on\", \"with\", \"at\", \"a\", \"an\"}\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def fetch_page(url):\n",
    "    \"\"\"Fetches a webpage using requests.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text  # Return HTML content\n",
    "        else:\n",
    "            print(f\"⚠️ Failed to fetch {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(html, base_url):\n",
    "    \"\"\"Extracts all valid links from a webpage.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = set()\n",
    "    \n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        full_url = requests.compat.urljoin(base_url, a_tag[\"href\"])  # Convert relative URLs to absolute\n",
    "        links.add(full_url)\n",
    "    \n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_illinois_links(links):\n",
    "    \"\"\"Keeps only links that belong to illinois.edu and its subdomains.\"\"\"\n",
    "    \"\"\"Keeps only links that belong to illinois.edu and all its subdomains.\"\"\"\n",
    "    \"\"\"Keeps only links that belong to illinois.edu domain.\"\"\"\n",
    "    return {link for link in links if requests.utils.urlparse(link).netloc.endswith(\".illinois.edu\") or requests.utils.urlparse(link).netloc == \"illinois.edu\"}\n",
    "\n",
    "\n",
    "def contains_keywords(html, keywords, min_count=3):\n",
    "    \"\"\"Extracts meaningful text and checks for AI-related keywords with improved accuracy.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    content_blocks = soup.find_all([\"article\", \"section\", \"div\", \"p\", \"h1\", \"h2\"], class_=True)\n",
    "    text = \" \".join(block.get_text() for block in content_blocks)\n",
    "    \n",
    "    # Clean and normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    words = text.lower().split()  # Tokenize text\n",
    "    words = [word for word in words if word not in STOPWORDS]  # Remove stopwords\n",
    "    \n",
    "    # Count keyword occurrences\n",
    "    matched_keywords = {kw: sum(1 for word in words if kw.lower() in word) for kw in keywords}\n",
    "    print(f\"🔍 Checking AI content on page... Found matches: {matched_keywords}\")\n",
    "    \n",
    "    return sum(matched_keywords.values()) >= min_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: https://illinois.edu\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   4%|▍         | 2/50 [00:01<00:37,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: http://library.illinois.edu\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   6%|▌         | 3/50 [00:03<00:53,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: http://grad.illinois.edu/admissions/apply\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   8%|▊         | 4/50 [00:04<01:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: https://forms.illinois.edu/sec/887006\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  10%|█         | 5/50 [00:06<01:03,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: http://grad.illinois.edu/diversity/enduring-transfer-pathways-graduate-education-stem\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  12%|█▏        | 6/50 [00:07<01:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: https://www.igb.illinois.edu/\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  14%|█▍        | 7/50 [00:09<01:03,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: http://grad.illinois.edu/professional-development/communication-skills\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  16%|█▌        | 8/50 [00:11<01:02,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning: https://www.library.illinois.edu/borrowing/overdue-and-lost-items/\n",
      "🔍 Checking AI content on page... Found matches: {'Deep Learning': 0, 'Neural Networks': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  16%|█▌        | 8/50 [00:12<01:06,  1.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m user_defined_sites \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://illinois.edu\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Users can specify multiple sites here\u001b[39;00m\n\u001b[0;32m     65\u001b[0m user_defined_keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeep Learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural Networks\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Users can add their own keywords\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m news_articles \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_illinois\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_defined_sites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_defined_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📌 AI-Related Articles Found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36mcrawl_illinois\u001b[1;34m(start_urls, user_keywords, max_pages)\u001b[0m\n\u001b[0;32m     38\u001b[0m             relevant_articles\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔹 Found AI article: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Be polite, don't overload the server\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Crawling complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m relevant_articles\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def crawl_illinois(start_urls, user_keywords=None, max_pages=100):\n",
    "    \"\"\"Crawls the Illinois website, looking for AI-related news articles.\"\"\"\n",
    "    visited = set()\n",
    "    to_visit = set(start_urls) if isinstance(start_urls, list) else {start_urls}\n",
    "    subdomains_visited = set()\n",
    "    relevant_articles = []\n",
    "    \n",
    "    with tqdm(total=max_pages, desc=\"Crawling pages\") as pbar:\n",
    "        while to_visit and len(visited) < max_pages:\n",
    "            if not to_visit:\n",
    "                break\n",
    "            url = to_visit.pop()\n",
    "            print(f'🔍 Scanning: {url}')  # Show the webpage being scanned\n",
    "            \n",
    "            if url in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(url)\n",
    "            pbar.update(1)\n",
    "\n",
    "            html = fetch_page(url)\n",
    "            if not html:\n",
    "                continue\n",
    "\n",
    "            links = extract_links(html, url)\n",
    "            illinois_links = filter_illinois_links(links)\n",
    "            to_visit.update(illinois_links - visited)\n",
    "            \n",
    "            for link in illinois_links:\n",
    "                domain = requests.utils.urlparse(link).netloc\n",
    "                if domain.endswith(\".illinois.edu\") and domain not in subdomains_visited:\n",
    "                    subdomains_visited.add(domain)\n",
    "                    to_visit.add(link)\n",
    "                elif link not in visited:\n",
    "                    to_visit.add(link)\n",
    "            \n",
    "            if contains_keywords(html, AI_KEYWORDS if not user_keywords else user_keywords):\n",
    "                relevant_articles.append(url)\n",
    "                print(f\"🔹 Found AI article: {url}\")\n",
    "\n",
    "            time.sleep(1)  # Be polite, don't overload the server\n",
    "    \n",
    "    print(\"\\n✅ Crawling complete!\")\n",
    "    return relevant_articles\n",
    "# Ask user if they want to add more websites\n",
    "def get_user_input(prompt):\n",
    "    user_input = input(prompt).strip()\n",
    "    return user_input if user_input.lower() != 'no' else None\n",
    "\n",
    "extra_sites = get_user_input(\"Would you like to add more websites? Enter them separated by commas or type 'no': \")\n",
    "if extra_sites:\n",
    "    user_defined_sites = [site.strip() for site in extra_sites.split(',')]\n",
    "else:\n",
    "    user_defined_sites = [\"https://illinois.edu\"]\n",
    "\n",
    "# Ask user if they want to add more keywords\n",
    "extra_keywords = get_user_input(\"Would you like to add more keywords? Enter them separated by commas or type 'no': \")\n",
    "if extra_keywords:\n",
    "    user_defined_keywords = [kw.strip() for kw in extra_keywords.split(',')]\n",
    "else:\n",
    "    user_defined_keywords = [\"Gen-AI\", \"Generative AI\", \"Artificial Intelligence\", \"Machine Learning\", \"AI\"]\n",
    "\n",
    "# Start crawling\n",
    "user_defined_sites = [\"https://illinois.edu\"]  # Users can specify multiple sites here\n",
    "user_defined_keywords = [\"Deep Learning\", \"Neural Networks\"]  # Users can add their own keywords\n",
    "\n",
    "news_articles = crawl_illinois(user_defined_sites, user_defined_keywords, max_pages=50)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📌 AI-Related Articles Found:\")\n",
    "for article in news_articles:\n",
    "    print(article)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
